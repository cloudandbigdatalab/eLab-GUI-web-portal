# Parallel and Distributed Computing Course

### Tools

The following cloud-based programming environment will be provided to each student: 
Python and Python libraries such as OpenMPI, numpy, scipy, ipython.

### Recommended Background

Students familiar with principle of programming should be successful in completing this course. The main aim of the course is to provide skills for Parallel and Distributed Computing.

### About the Course

Over the last years, high performance computing has become an affordable resource to many more researchers in the scientific community than ever before. The conjunction of quality open source software and commodity hardware strongly influenced the now widespread popularity of cluster computing.

Among many parallel computational models, message-passing has proven to be an effective one. This paradigm is specially suited for distributed memory architectures and is used in todayâ€™s most demanding scientific and engineering application related to modeling, simulation, design, and signal processing. However, portable message-passing parallel programming used to be a nightmare in the past because of the many incompatible options developers were faced to. Fortunately, this situation definitely changed after the MPI Forum released its standard specification.

MPI, the Message Passing Interface, is a standardized and portable message-passing system designed to function on a wide variety of parallel computers. The standard defines the syntax and semantics of library routines and allows users to write portable programs in the main scientific programming languages (Fortran, C, or C++).


### Syllabus

- Introduction to Parallel and Distributed Computing.
- Basics of MPI. 
- Executing our 1st python MPI program.
- Point to Point Communication.
- Parallelizing the Trapezoidal Rule.
- Load Balancing.
- Collective Communication.
- Reduce() and Allreduce().
- Scatterv() and Gatherv()
